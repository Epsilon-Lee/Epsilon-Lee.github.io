---
layout: post
title: Mode Regularization for GANs
author: Guanlin Li
tag: archive
---

> This article list a few recent papers on neural machine translation, where new state-of-the-art have been made. So I am aiming at organize some experiment settings: including dataset, model architecture comparison and hyper-parameters selection so as to facilitate further experiment. 

Recently the most notable and solid works or breakthroughs on neural machine translation are almost all made by big AI companies, Google's GNMT, Transformer, Facebook's Conv2Seq. Those models provide stable improvements over same datasets towards reaching human-like performance in general. So I choose the following papers as the current state-of-the-art not only because their high test BLEU score, but their sufficient experiments. 

- (**GNMT**)[Google's Neural Machine Translation System: Briding the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf), Sep. 2016. 
- (**Massive**)[Massive Exploration of Neural Machine Translation Architectures](https://aclweb.org/anthology/D17-1151), ACL 2017. 
- (**ConvEnc**)[A Convolutional Encoder Model for Neural Machine Translation](https://www.aclweb.org/anthology/P/P17/P17-1012.pdf), ACL 2017. 
- (**ConvS2S**)[Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122), ICML 2017. 
- (**T2T**)[Attention is All You Need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf), NIPS 2017. 

### 1. Dataset

| Paper   | Training Set                      | Dev            | Test    |
| ------- | --------------------------------- | -------------- | ------- |
| GNMT    | WMT14 En->Fr (36M), En->De (5M)   | nt12+13 (6003) | nt14    |
| Massive | WMT15 En->DE (4.5M)               | nt13           | nt14+15 |
| ConvEnc | WMT15 En->De (4.9M), En->Fr (12M) | ?              | nt15    |
| ConvS2S | WMT14 En->De (4.5M), En->Fr (36M) | ?              | nt14    |
| T2T     | WMT14 En->De (4.5M), En->Fr (46M) | nt13           | nt14    |

> - ConvEnc: En->Fr remove 150 longer sentences to reduce 12M to 10.7M. 
