---
layout: post
title: "Neural Machine Translation: State of the Art"
author: Guanlin Li
tag: archive
---

> This article list a few recent papers on neural machine translation, where new state-of-the-art have been made. So I am aiming at organize some experiment settings: including dataset, model architecture comparison and hyper-parameters selection so as to facilitate further experiment. 

Recently the most notable and solid works or breakthroughs on neural machine translation are almost all made by big AI companies, Google's GNMT, Transformer, Facebook's Conv2Seq. Those models provide stable improvements over same datasets towards reaching human-like performance in general. So I choose the following papers as the current state-of-the-art not only because their high test BLEU score, but their sufficient experiments. 

- (**GNMT**) [Google's Neural Machine Translation System: Briding the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf), Sep. 2016. 
- (**Massive**) [Massive Exploration of Neural Machine Translation Architectures](https://aclweb.org/anthology/D17-1151), ACL 2017. 
- (**ConvEnc**) [A Convolutional Encoder Model for Neural Machine Translation](https://www.aclweb.org/anthology/P/P17/P17-1012.pdf), ACL 2017. 
- (**ConvS2S**) [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122), ICML 2017. 
- (**T2T**) [Attention is All You Need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf), NIPS 2017. 

### 1. Dataset

| Paper   | Training Set                      | Dev            | Test    |
| ------- | --------------------------------- | -------------- | ------- |
| GNMT    | WMT14 En->Fr (36M), En->De (5M)   | nt12+13 (6003) | nt14    |
| Massive | WMT15 En->De (4.5M)               | nt13           | nt14+15 |
| ConvEnc | WMT15 En->De (4.9M), En->Fr (12M) | ?              | nt15    |
| ConvS2S | WMT14 En->De (4.5M), En->Fr (36M) | ?              | nt14    |
| T2T     | WMT14 En->De (4.5M), En->Fr (46M) | nt13           | nt14    |

> - ConvEnc: En->Fr remove 150 longer sentences to reduce 12M to 10.7M. 

### 2. Vocabulary and UNK techniques

| Paper   | Vocabulary/Lexicon construction strategy |
| ------- | ---------------------------------------- |
| GNMT    | En-Fr: Word 212k-src 80k-tgt, Word pieces (8k, 16k, 32k), mixed word/char model (32k) |
| Massive | Shared subword units by BPE, 32000 merge vocab size ~= 37k |
| ConvEnc | En-Fr: Word 200k-src 30k-tgt; En-De: Word 200k-src 80k-tgt |
| ConvS2S | En-De: BPE 40k, En-Fr: BPE 40k           |
| T2T     | En-De: BPE 37k, En-Fr: word pieces 25k   |

> - Reference for learning word pieces (which is a model-based lexicon learning approach). 
>   - [Japanese and Korean Voice Search](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf), ICASSP 2012. 

| Paper   | UNK techniques                           |
| ------- | ---------------------------------------- |
| GNMT    | Attention-based src-to-tgt copy and replace |
| Massive | ?                                        |
| ConvEnc | Attention-based src-to-precomputed-dict (fast align) and replace |
| ConvS2S | Same as ConvEnc                          |
| T2T     | ?                                        |

> - Positional unknown word model (PosUnk) is first introduced in 

### 3. Optimization algorithm or procedure

| Paper   | Optimization                             |
| ------- | ---------------------------------------- |
| GNMT    | Adam first 60k updates (lr=2*e-4), then SGD (lr=0.5) anneal half every 1.2M updates, 5 gradient clip |
| Massive | Adam lr=e-4, dropou=0.2, batch=128, 2.5M updates 4 times |
| ConvEnc | RNN: Adam lr=3.125*e-4, early stop w.r.t. dev ppl; Conv: SGD with annealing, lr=0.1, batch=64 (32), 25 gradient clip, dropout=0.1 (0.2) |
| ConvS2S | Nesterov 0.99 momentum 0.1 gradient clip; lr start from 0.25, decay w.r.t. dev ppl, util below e-4; mini-batch 64 |
| T2T     | Batch token: 25000; Adam (lr decay with warmup) |

### 4. Beam search strategy

| Paper   | Beam search                              |
| ------- | ---------------------------------------- |
| GNMT    | coverage penalty, length penalty (8-12 beam size) |
| Massive | 10 beam size, same penalty as in GNMT    |
| ConvEnc | length normalization by length $$\vert Y \vert$$ |
| ConvS2S | 5 beam size, length normalization (by length $$\vert Y \vert$$ or tuned as in GNMT) |
| T2T     | 4 beam size length penalty $$\alpha=0.6$$, maximum length is 50 |

> - Coverage penalty and length penalty
>   - $$s(X, Y) = \log P(Y \vert X) / lp(Y) + cp(X; Y)$$
>   - $$lp(Y) = \frac{(5 + \vert Y \vert)^\alpha}{(5 + 1)^\alpha}$$
>   - $$cp(X; Y) = \beta \times \sum_{i=1}^{\vert X \vert} \log (\min (\sum_{j=1}^{\vert Y \vert} p_{i, j}, 1.0))$$
> - It seems that length penalty can slightly improve BLEU score in the above models/papers, however, Phillip Koehn's challenge paper has shown the vagueness of this strategy could result in coherent improvements. 
>   - [Six Challenges for Neural Machine Translation](https://arxiv.org/abs/1706.03872), Jun. 2017. 

### 5. BLEU comparison

This part gives some BLEU comparisons on two standard dataset:

- EN->FR 36M
- EN->DE 4~5M


| Paper & Model           | EN->DE     | EN->FR         |
| ----------------------- | ---------- | -------------- |
| GNMT WPM-32K            | 24.61      | 38.95          |
| Massive                 | ?          | ?              |
| ConvEnc 2-layer Decoder | 24.2 (80K) | 35.7 (12M 30K) |
| ConvS2S BPE40K          | 25.16      | 40.51          |
| T2T base                | 27.3       | 38.1           |
| T2T big                 | 28.4       | 41.0           |

