#### Response to Reviewer #1

"The idea of using it (barriers) for NMT reranking actually does not..."

Yes, as you noticed, the reranking candidates is not generated by editing the barrier words, since we haven't come up with a method for detecting barriers without knowing the reference yet. Actually, it is the discovery of barrier words (we can generate much better translation candidates by editing one single source words) that motivates us to propose to edit **every** source words with certain amount of random tokens in the vocabulary to generate better oracle candidates for reranking.



"More explanation is wanted for context-aware and model-aware and previous methods are not."

Previous methods are based on global statistics (calculated once from the whole training set) like word frequency, translation entropy (obtained from IBM model, instead of the NMT model), so the rank of every two word is **fixed** no matter what specific sentence they co-occurs in, therefore, we call those words recognized by global statistics as model-agnostic/context-agnostic or global troublemakers. However, in our definition of generalization barriers, it is highly context-dependent, since for NMT, not like the IBM model family assuming factorization of translation probability, the input words/phrases's representations are highly entangled by the encoder, this might be one reason for the context-sensitivity of the detected barrier words.



We will fix other typos in the next revised version. Thanks for all the advices, we really appreciated them!	



#### Response to Reviewer #2

"The collected words may be biased and noisy. There are no discussion whether the collected words are representative"

Definition 3.1 of barrier words **ensures** that those words are the cause of performance degradation on certain input x, since by modifying it, the performance will witness significant boost w.r.t. human evaluator, which actually meets the representativeness principle in your comment. Since it is very expensive for asking human to edit and judge improvement, we have to carefully adopt automatic metrics (like sentence-BLEU) for large-scale analysis. 



"translation error... Unknown words like rare named entities..."

The focus of our investigation is on the cause (token) in the input that due to its existence makes the NMT model translates badly. One point to clarify here is that, our definition (3.1) does not cover those source words that is **itself** not well-translated, but those source words, due to their existence, becoming the cause of mis/under-translation of other source words thus hindering the overall generalization of this instance.



"The usage of the word 'generalization' seems to be strange"

Generalization is the ability of a trained model to generalize on unseen data, it should be tested on held-out data. Training data is one factor that determines its generalization ability (together with inductive bias of the model, optimization and so on), but we should not use training data for understanding model's weakness and crispness of generalization. So we think it is proper for naming those tokens in held-out sentences as generalization barriers. If you focus on detecting barriers with training data, it is to test the **fitting/memorization** ability (not the generalization ability) of the trained model, since the model has already seen those data and might have memorized the ground-truth mapping.



"Definition 3.1 is vague."

We would like to clarify that a principled definition is not necessary a formal definition with mathematical instantiation of every details, but a definition that is convincing and makes the underlying concept acceptable and general enough. But we would like to modify it to be more formal and close to our second definition of barrier words. 



#### Response to Reviewer #3

"the method relies on human to improve performance"

The main purpose of this paper is to detect the so-called generalization barrier words for better understanding the current weakness and crispness of NMT model at a very fine-grained level of causality. We think it is the most important issue of current large-scale supervised NMT systems, that is, corpus-level BLEU is uninformative for us to find problems. Instead of just telling the average corpus BLEU score, we can analyze what on earth cause the NMT model to perform poor on a specific instance. So we propose two definitions of generalization barriers, and used the approximate definition which **does not** involve human effort for automatic detection of barriers. Our main purpose is not to propose specific strategies for improving translation performance, but to characterize how and why the model perform poor on some test instances.



#### General Response to Reviewers

Thank you all for your time for reviewing our paper and your precious critics. Here we would like to reiterate our contributions and position:

1. We find and formalize a phenomenon that by simply making tiny adjustment of the input, the model used to perform poorly can then bring significant better translation: this could be thought of as a reverse phenomenon of the well-known adversarial examples.
2. We give operational definition to those adjusted input words as generalization barrier words, which we believe once we have a deeper understanding of **why** they cause the model fail to generalize well can benefit us from focusing on solving the real crispness of current NMT (from model design to training).
3. We conduct several descriptive (quantitative+qualitative) understanding of the detected barrier words to characterize its linguistic properties, its differences from other well-established source-side trouble makers.

By and large, our work could be set in the position as a new angle of fine-grained evaluation that gives informative performance descriptions of the NMT model for better understanding its weakness.



**Response to Chairs**

Dear chairs,

We have a few concerns about the appropriateness of the reviews we get. 

Regarding reviewer 3, we think s/he might substantially miss the main contribution of our work, we haven't rely on human to improve translation performance, essentially we are focusing on better characterizing and understanding some mis-behaviors of the NMT model.

Regarding reviewer 2, we think s/he has his own recognition and focus on the subject we are dealing with, most of his comments really matter for future work along this line (such as really taking compositionality into account), but w.r.t. our concern of the barrier phenomenon and better evaluation for finding current NMT's weakness, we think this work is a call and a well-established investigation for the first step.

We hope you may re-assess the position of our work. Thanks a lot.