---
layout: post
title: "Expectation, Maximization."
author: Guanlin Li
tag: notes
---

> This is a history file of the expectation-maximization theory or algorithm. 

### Prelude

Expectation-maximization (EM) has puzzled me ever since my first graduate year on the Pattern Recognition or Machine Translation courses, i.e. Gaussian mixture model (GMM), IBM translation models (IBM Models) etc. It is now my third year, and I still could not apply it to solve estimation problems in probabilistic modeling flexibly. 

With a blog-writing while learning style, I try to generatively derive the basic idea of EM through GMMs and IBMs, each of which belongs to continuous or discrete observation domain respectively.  

#### Gaussian mixture models (frequentist form)

> **Model description.**
>
> Given $$k$$ components, and $$(\mu_i, \sigma_i)$$ for each component with mixture probability $$pi_i$$; denote $$\theta$$ for all mean, variance pairs and the mixture probabilities. 
>
> $$p(x; \theta) = \sum_i \pi_i \mathcal{N}(x; \mu_i, \sigma_i)$$
>
> When observe $$N$$ data points $$x_{1:N}$$, estimate $$\theta$$ through maximum likelihood principle. 
>
> **Gaussian density.**
>
> - Univariate: $$\mathcal{N}(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \big{(} -\frac{(x-\mu)^2}{2\sigma^2}  \big{)}$$
> - Multivariate: $$\mathcal{N}(x; \mu, \Sigma) = \frac{1}{\sqrt{(2\pi)^k \vert \Sigma \vert}} \exp\big{(} -\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x-\mu) \big{)}$$

At first sight, we can have the following optimization problem (considering the univariate situation). 

$$
\begin{align}
\arg \max_\theta \sum_i \log p(x; \theta) &= \arg \max_\theta \sum_i \log \sum_i \pi_i \mathcal{N}(x; \mu_i, \sigma_i)
\end{align}
$$

Let us try to compute the gradient w.r.t. $$\pi_i$$ and $$\mu_i$$ respectively. 

$$
\begin{align}
& \nabla_{\pi_j} \log \sum_i \pi_i \cdot \mathcal{N}_i \\
=& \frac{1}{\mathcal{L}(x)} \nabla_{\pi_j} \sum_i \pi_i \cdot \mathcal{N_i} \\
=& \frac{1}{\mathcal{L}(x)} \cdot \mathcal{N}_j
\end{align}
$$

